version: "3.9"
services:
  minio-server:
    image: minio/minio:latest
    profiles: ['data_persistence_layer']
    hostname: minio-server
    container_name: minio-server
    networks:
      default:
    volumes:
      - ./s3_data:/data
    ports:
      - "9000:9000"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data

  minio-client:
    image: minio/mc
    profiles: ['data_persistence_layer']
    hostname: minio-client
    container_name: minio-client
    networks:
      default:
    depends_on:
      - minio-server
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc config;
      /usr/bin/mc config host add myminio ${S3_ENDPOINT_URL} ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb --ignore-existing myminio/${S3_BUCKET_NAME};
      exit 0;
      "

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    profiles: ['data_persistence_layer']
    hostname: zookeeper
    container_name: zookeeper
    networks:
      default:
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:7.3.0
    profiles: ['data_persistence_layer']
    hostname: broker
    container_name: broker
    networks:
      default:
    depends_on:
      - zookeeper
    ports: 
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://${KAFKA_SERVER}

  kafka-rest:
    image: confluentinc/cp-kafka-rest:7.3.0
    profiles: ['data_persistence_layer']
    hostname: kafka-rest
    container_name: kafka-rest
    networks:
      default:
    depends_on:
      - broker
    ports:
      - "8082:8082"
    environment:
      KAFKA_REST_HOST_NAME: kafka-rest
      KAFKA_REST_BOOTSTRAP_SERVERS: ${KAFKA_SERVER}
      KAFKA_REST_LISTENERS: ${KAFKA_REST_API_URL}

  s3-connector:
    image: streamthoughts/kafka-connect-file-pulse:2.5.0
    profiles: ['data_persistence_layer']
    hostname: s3-connector
    container_name: s3-connector
    networks:
      default:
    depends_on:
      - broker
      - minio-server
    ports:
    - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: ${KAFKA_SERVER}
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 1000
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: s3-connector
      CONNECT_REST_PORT: 8083
    volumes:
      - ./s3_connector:/home/appuser
    command: 
      - bash 
      - -c 
      - |
        #
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run & 
        #
        echo "Waiting for Kafka Connect to start listening on localhost â³"
        while : ; do
          curl_status=$$(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors)
          echo -e $$(date) " Kafka Connect listener HTTP state: " $$curl_status " (waiting for 200)"
          if [ $$curl_status -eq 200 ] ; then
            break
          fi
          sleep 5 
        done
        echo -e "\n--\n+> Creating s3-connector"
        curl -s -X PUT -H  "Content-Type:application/json" http://localhost:8083/connectors/s3-connector/config \
            -d @s3-connector-config.json
        sleep infinity
    
  sensorsmock:
    build:
      context: ./sensors
    profiles: ['data_source']
    hostname: sensorsmock
    container_name: sensorsmock
    networks:
      default:
    depends_on:
      - minio-server
      - kafka-rest
    ports:
      - "3000:3000"
    environment:
      - SMART_THERMO_BUCKET=${S3_BUCKET_NAME}
      - MOISTURE_MATE_URL=${KAFKA_REST_API_URL}/topics/${MOISTURE_MATE_TOPIC_NAME}
      - CARBON_SENSE_URL=${KAFKA_REST_API_URL}/topics/${CARBON_SENSE_TOPIC_NAME}
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - ENDPOINT_URL=${S3_ENDPOINT_URL}

  luxmeter-extractor:
    build:
      context: ./luxmeter_extractor
    profiles: ['data_transport_layer']
    hostname: luxmeter-extractor
    container_name: luxmeter-extractor
    networks:
      default:
    depends_on:
      - sensorsmock
      - broker
    environment:
      - KAFKA_SERVER=${KAFKA_SERVER}
      - KAFKA_TOPIC=${LUXMETER_TOPIC_NAME}
      - LUXMETER_ENDPOINT=${LUXMETER_ENDPOINT}
      
  transformation-service:
    build:
      context: ./transformation_service
    profiles: ['data_transformation_layer']
    hostname: transformation-service
    container_name: transformation-service
    networks:
      default:
    depends_on:
      - broker
    environment:
      - KAFKA_SERVER=${KAFKA_SERVER}
      - CARBON_SENSE_TOPIC=${CARBON_SENSE_TOPIC_NAME}
      - MOISTURE_MATE_TOPIC=${MOISTURE_MATE_TOPIC_NAME}
      - LUXMETER_TOPIC=${LUXMETER_TOPIC_NAME}
      - SMART_THERMO_TOPIC=${SMART_THERMO_TOPIC_NAME}
      - OUTPUT_TOPIC=${TRANSFORMED_DATA_TOPIC_NAME}
  
  prediction-service:
    build:
      context: ./prediction_service
    profiles: ['data_prediction_layer']
    hostname: prediction-service
    container_name: prediction-service
    networks:
      default:
    depends_on:
      - broker
    environment:
      - KAFKA_SERVER=${KAFKA_SERVER}
      - CONSUMER_TOPIC=${TRANSFORMED_DATA_TOPIC_NAME}
      - PRODUCER_TOPIC=${PREDICTED_DATA_TOPIC_NAME}
      - MODEL_FILENAME=${MODEL_FILE}
    
  load-service:
    build:
      context: ./load_service
    profiles: ['data_load_layer']
    hostname: load-service
    container_name: load-service
    networks:
      default:
    depends_on:
      - broker
    environment:
      - KAFKA_SERVER=${KAFKA_SERVER}
      - KAFKA_TOPIC=${PREDICTED_DATA_TOPIC_NAME}
      - MONGODB_CONNECTION_URL=${MONGODB_CONNECTION_URL}
      - MONGODB_DATABASE=${MONGODB_DATABASE}
      - MONGODB_COLLECTION=${MONGODB_COLLECTION}
  
  frontend:
    build:
      context: ./frontend
    ports:
      - "5000:5000"
    profiles: ['frontend_layer']
    hostname: frontend
    container_name: frontend
    networks:
      default:
    environment:
      - APP_SECRET=${APP_SECRET}
      - MONGODB_CONNECTION_URL=${MONGODB_CONNECTION_URL}
      - MONGODB_DB_NAME=${MONGODB_DATABASE}
      - MONGODB_COL_NAME=${MONGODB_COLLECTION}
      
networks:
  default:
    name: capstone-project
